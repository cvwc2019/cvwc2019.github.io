<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Challenge-CVWC2019</title>

  <link rel="stylesheet" type="text/css" href="./bootstrap.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto+Slab:400,700|Roboto:300,300i,400,400i,500,500i,700,700i">
  <link rel="stylesheet" type="text/css" href="./main.css" media="screen,projection">
  <link rel="icon" href="./icon.png" size="32x32">
</head>
<body>
  <div class="navbar navbar-default navbar-fixed-top">
    <div class="container">

      <div class="navbar-header">
      </div>

      <div class="navbar-collapse collapse" id="navbar-main">
        <ul class="nav navbar-nav">
          <li><a href="./index.html#body-home">Home</a></li>
          <li><a href="./challenge.html">Challenge</a></li>
          <li><a href="./cfp.html">Call for Papers</a></li>
          <li><a href="./people.html">People</a></li>
          <li><a href="./leaderboard.html">LeaderBoard</a></li>
        </ul>
      </div>

    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <h2>Overview</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <p>
          The challenge will explore the use of CV techniques for endangered wildlife conservation, specifically focusing on the
		<a href="https://en.wikipedia.org/wiki/Siberian_tiger" target="_blank">Amur tiger</a>, also known as the Siberian tiger
		or the Northeast-China tiger.  The Amur tiger population is concentrated in the Far East, particularly the Russian Far
		East and Northeast China. The remaining wild population is estimated to be 600 individuals, so conservation is of crucial
		importance.
        </p>
<p><u>Dataset:</u> With the help of WWF, a third-party company (MakerCollider) collected more than 8,000 Amur tiger video clips of 92 individuals from ~10 zoos in China. We organize efforts to make bounding-box, keypoint-based pose, and identity annotations for sampled video frames and formuate the ATRW (Amur Tiger Re-identification in the Wild) dataset.
<span style='color: #0000FF'>Figure 1</span> illustrates some example bounding box and pose keypoint annotations in our ATRW dataset.
        Our dataset is the largest wildlife re-ID dataset to date, <span style='color: #0000FF'>Table 1</span> lists a comparison of current wildlife re-ID datasets.
        The dataset will be divided into training, validation, and testing subsets.
        The training/validation subsets along with annotations will be released to public, with the annotations for the test subset withheld by the organizers. The dataset paper is released on Arxiv: <a href="https://arxiv.org/abs/1906.05586", target="_blank">1906.05586</a>.
        </p>
<p><u>Dataset Copyright:</u> The whole dataset is released under the non-commercial/research purposed <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/", target="_blank"><span style='color: #0000FF'>CC BY-NC-SA 4.0 Lcense</span></a>, with MakerCollider and WWF Amur tiger and leopard conservation programme team keeping the copyright of the raw video clips and all derived images.
</P>
    <div class="col-xs-12 image_container">
      <img src="./imgs/det_1.jpg"/> <img src="./imgs/det_3.jpg"/>
      <img src="./imgs/pose_3.jpg"/> <img src="./imgs/pose_4.jpg"/>
      <!-- <img src="./imgs/det_3.jpg"/> <img src="./imgs/det_4.jpg"/> -->
    </div>
    <center>Figure 1. Illustration of bounding box and pose keypoint annotations in our ATRW dataset.</center>
    <p><br /> </p>
     <div class="col-xs-12">
        <table style="width:100%;border-bottom:solid 2px">
          <thead style="border-bottom:solid 2px;border-top:solid 2px">
            <tr>
              <th>Datasets</th>
              <th> <u>ATRW</u> </th>
              <th>[1,2]</th>
              <th>C-Zoo[3]</th>
              <th>C-Tai[3]</th>
              <th>TELP[4]</th>
              <th>α-whale[5]</th>
            </tr>
          </thead>
          <tr>
            <th>Target</th><td> Tiger </td><td> Tiger </td><td> Chimpanzees </td><td> Chimpanzees </td><td> Elephant </td><td> Whale</td>
          </tr>
          <tr>
            <th>Wild</th><td>√</td><td> √ </td><td> × </td><td>×  </td><td>× </td><td> √</td>
          </tr>
          <tr>
            <th>Pose annotation  </th><td>√</td><td> × </td><td>×</td><td>×  </td><td>× </td><td> ×</td>
          </tr>
          <tr>
            <th>#Images or #Clips   </th><td> 8,076* </td><td> -   </td><td> 2,109  </td><td> 5,078  </td><td> 2,078 </td><td> 924</td>
          </tr>
          <tr>
            <th>#BBoxes</th><td> 9,496 </td><td> -   </td><td> 2,109  </td><td> 5,078 </td><td>  2,078 </td><td> 924</td>
          </tr>
          <tr>
            <th>#BBoxes with ID</th><td>  3,649</td><td> - </td><td> 2,109  </td><td> 5,078 </td><td>  2,078 </td><td> 924</td>
          </tr>
          <tr>
            <th>#identities  </th><td>  92   </td><td> 298   </td><td> 24     </td><td> 78    </td><td>  276   </td><td> 38</td>
          </tr>
          <tr>
            <th>#BBoxes/ID   </th><td>  39.7 </td><td> -     </td><td> 19.9   </td><td> 9.7   </td><td>  20.5  </td><td> 24.3</td>
          </tr>
        </table>
    </div>
    <center>Table 1. Comaprison of current wildlife re-ID datasets.</center>
    <p><br /> </p>
        <p><u>Requirement:</u> We require that participants agree to open-source their solution to support wildlife conservation.
        Participants allow using pre-trained models on ImageNet, COCO, etc for the challenge. They should clearly state what kind of pre-trained models are used in their submission. Using dataset with tiger category and additional collected tiger data are not allowed.
        Participants should submit their challenge results as well as full source-code packages for evaluation before deadline.
        </p>
      </div>
    <p><br /></p>
</div>

<div class="row">
 <div class="col-xs-12">
  <h2>Tracks</h2>
 </div>
</div>
<div class="row">
      <div class="col-xs-12">
        <p>
          <u>Tiger Detection</u>: From images/videos captured by cameras, this task aims to place tight bounding boxes around tigers. As the detection may run on the edge (smart cameras), both the detection accuracy (in terms of AP) and the computing cost are used to measure the quality of the detector.
        </p>
        <p>
          <u>Tiger Pose Detection</u>: From images/videos with detected tiger bounding boxes, this task aims to estimate tiger pose (i.e., keypoint landmarks) for tiger image alignment/normalization, so that pose variations are removed or alleviated in the tiger re-identification step. We will use mean average precision (mAP) and object keypoint similarity (OKS) to evaluate submissions.
        </p>
        <p>
          <u>Tiger Re-ID with Human Alignment (Plain Re-ID)</u>: We define a set of queries and a target database of Amur tigers. Both queries and targets in the database are already annotated with bounding boxes and pose information. Tiger re-identification aims to find all the database images containing the same tiger as the query. Both mAP and rank-1 accuracy will be used to evaluate accuracy.
        </p>
        <p>
          <u>Tiger Re-ID in the Wild</u>: This track will evaluate the accuracy of tiger re-identification in wild with a fully automated pipeline. To simulate the real use case, no annotations are provided. Submissions should automatically detect and identify tigers in all images in the test set. Both mAP and rank-1 accuracy will be used to evaluate the accuracy of different models.
        </p>
      </div>
     <p><br /></p>
    </div>

<div class="row">
 <div class="col-xs-12">
  <h2>Awards</h2>
 </div>
</div>
<div class="row">
      <div class="col-xs-12">
		<p>The workshop will provide awards for each challenge track winner team thanks to our sponsor's generous donation.
		Detailed award info will be available soon. </p>
      </div>
     <p><br /></p>
     <p><br /></p>
</div>

<style>
a{
  color:#337ab7;
}
th,td{
  padding-left:5px;
  padding-right: 5px;
}
th{
  text-align: center;
}
</style>

<div class="row">
 <div class="col-xs-12">
  <h2>Dataset Download</h2>
 </div>
</div>
 <br>
<div class="row">
  <div class="col-xs-12">
    <table border=1px cellpadding=200>
      <thead style="font-size:1.5em;">
        <th>Track</th><th>Split</th><th>Images</th><th>Annotations</th>
      </thead>
      <tbody>
        <tr>
          <td rowspan="2">Detection</td>
          <td>train</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_detection_train.tar.gz" target="_blank">Dectection_train</a> </td>
          <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_anno_detection_train.tar.gz" target="_blank">Anno_Dectection_train</a> </td>
        </tr>
        <tr>
          <td>test</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/test/atrw_detection_test.tar.gz" target="_blank">Detection_test</a> </td> <td> <a href="#">-</a> </td>
        </tr>
        <tr>
          <td  rowspan="2">Pose</td>
          <td>train</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_pose_train.tar.gz" target="_blank">Pose_train</a>,<a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_pose_val.tar.gz", target="_blank">Pose_val</a> </td>
          <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_anno_pose_train.tar.gz" target="_blank">Anno_Pose_trainval</a> </td>
        </tr>
        <tr>
           <td>test</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/test/atrw_pose_test.tar.gz" target="_blank">Pose_test</a> </td> <td> <a href="#">-</a> </td>
        </tr>
        <tr>
          <td  rowspan="2">Plain Re-ID</td>
          <td>train</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_reid_train.tar.gz" target="_blank">ReID_train</a> </td>
          <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/train/atrw_anno_reid_train.tar.gz" target="_blank">Anno_ReID_train</a> </td>
        </tr>
        <tr>
           <td>test</td> <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/test/atrw_reid_test.tar.gz" target="_blank">ReID_test</a> </td>
           <td> <a href="https://lilablobssc.blob.core.windows.net/cvwc2019/test/atrw_anno_reid_test.tar.gz">Anno_ReID_test(Keypoint ground truth + test image list)</a> </td>
        </tr>
        <tr>
          <td  rowspan="">Re-ID in the Wild</td>
          <td>test</td> <td> same as detection test set </td> <td> <a href="#">-</a> </td>
        </tr>
      </tbody>
    </table>
  </div>
  <p><br /></p>
  <span>&emsp;*Data hosting provided by Microsoft AI for Earth Program, Thanks.</span>
</div>
<p><br /></p>
<div class="row">
 <div class="col-xs-12">
  <h2>Format Description</h2>
 </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
      <li> <u>Detection</u>: Data annotaiton in <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html", target="_blank">Pascal VOC</a> format. Submission in <a href="http://cocodataset.org/#format-data", target="_blank">COCO detection</a> format. Training with the given training set and testing set will be provided in the test stage.
      </li>
      <li> <u>Pose</u>: Both data annotaiton and submission are in <a href="http://cocodataset.org/#format-data", target="_blank">COCO</a> format.
      Training with the given training set and testing set will be provided in the test stage.
      </li>
      <li> <u>Plain ReID</u>: Dataset contains cropped images with manual annotaetd ID and keypoints. Submission should be a json file in the following format:</li>
      <br>
        <p>
          [<br>
            &emsp;{"query_id":0,
            &emsp; "ans_ids":[29,38,10,.......]},<br>
            &emsp;{"query_id":3,
            &emsp; "ans_ids":[95,18,20,.......]},<br>
            &emsp;...<br>
          ]<br>
          <span style='color: #0000FF'>where</span> the "query_id" is the id of query image, and each followed array "ans_ids" lists re-ID results (image ids) in the confidence descending order.
          <br>
	       Similar to most existing Re-ID tasks, the plain Re-ID task requires to build models on training-set, and evaluating on the test-set.
	       During testing, each image will be taken as query image, while all the remained images in the test-set as "gallery" or "database", the query results should be rank-list of images in "gallery". The evaluation server will separate the test-set into two cases: single-camera and cross camera (see our arxiv report for more details) to measure performance. The evaluation metrics are mAP and top-k (k=1, 5).
        </p>
      <li> <u>ReID in Wild</u>: This task aims to evaluate the performance of Re-ID in a full automatical way.
      Paritipants require to build tiger detector, tiger pose estimator, and re-ID module based on the provide training-set, and integrate them as a full pipeline
      to re-identification each detected tiger in a set of wild input images.
      The test-set is the same as that of the detection task. The re-ID evaluation will use all the detected boxes as "gallery", while the other procedure is smilar to the plain re-ID case. Submission should be a json file with the following schema:<br><br>
        {<br>
          &emsp;"bboxs":[bbox],<br>
          &emsp;"reid_result":[<br>
          &emsp;&emsp;{"query_id":int,&emsp;"ans_ids":[int,int,...]}<br>
          &emsp;]<br>
        }
        <br>
	      <span style='color: #0000FF'>where</span>
		<br>
        bbox{<br>
          &emsp;"bbox_id": int, #used in reid_result<br>
          &emsp;"image_id": int,<br>
          &emsp;"pos": [x,y,w,h] #x,y is the top-left coord, all in pixels.<br>
        }<br>
        <span style='color: #0000FF'>where</span>  the 'reid_result' is almost the same format as in Plain ReID, with only 'id' replaced by 'bbox_id'.
        To make the evaluation tackable, partcipants require to make the 'bbox_id' unique in their submission file not only in each test image, but also among all images in the test-set.
      </p>
    </ul>
  </div>
</div>

<div class="row">
 <div class="col-xs-12">
  <h2>Evaluation</h2>
 </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p><a href="https://evalai.cloudcv.org/web/challenges/challenge-page/412/overview" target="_blank">Evaluation server</a> is now opened. Thanks EvalAI for the support. Note please choose correct track (or phase) during results submission.</p>
  </div>
</div>

<div class="row">
 <div class="col-xs-12">
  <h2>FAQ</h2>
 </div>
</div>
<div class="row">
<div class="col-xs-12">
  <ul>
    <li><b>Q:</b> Do we require registration to attend? <br>
        <b>A:</b> No. But, you are required to register to submit results on the evaluation server, which will be released soon.</b>
    </li>
    <li><b>Q:</b> Detection/keypoint format examples? <br>
        <b>A:</b> Please refer to <a href='https://github.com/cocodataset/cocoapi/blob/master/results/instances_val2014_fakebbox100_results.json', target="_blank">this file</a> for detection and
        <a href="https://github.com/cocodataset/cocoapi/blob/master/results/person_keypoints_val2014_fakekeypoints100_results.json", target="_blank">this file</a> for keypoint. Note that the keypoint definition is different in COCO and our datasets, please make corresponding changes.
    </li>
    <li><b>Q:</b> In plain reID task, 3392 cropped images not match to 1887 annotations in training set?<br>
        <b>A:</b> The annotated images include three cases: left/right sides, and frontal tiger face.
        Our released annotation only contains left/right side pose annotation of tigers since tiger re-id is based on stripe pattern rather than tiger face. <b>For the challenge, please build system based on the annotation file.</b>
    </li>
    <li><b>Q:</b> Could you provide evaluation tool? <br>
        <b>A:</b> We are preparing and testing the evaluation server, which should be ready before we release testing dataset.
         Please keep an eye on our website for the update. For now, you can use following scripts to test on validate set. <br>
        -Detection: standard COCOApi (python ver.).<br>
        -Keypoint:  modified COCOApi (python ver.). Please change the 'sigmas' in L206 or L523 (depends on the cocoapi version) of cocoeval.py according to our paper (in which squared sigmas are listed).<br>
        -Plain ReID: Please refer to evaluate.py of <a href="https://github.com/VisualComputingInstitute/triplet-reid", target="_blank">this repo</a>.
        As the frame excluder are not open to public, the evluation results may tend to be optimistic.
    </li>
    <li><b>Q:</b> What is difference between open testing leaderboard, and final evaluation leaderboard?<br>
        <b>A:</b> We adopt similar strategy as some kaggle challenges. During open testing, we randomly select half samples (fixed) from testset for evaluation, and open this leaderboard to public. The final leaderboard will be based on the evaluation over the whole testset, which will be released to public after the challenge. </b>
    </li>
    <li><b>Q:</b> How many submissions per day during testing phase?<br>
        <b>A:</b> Each participant team will allow at most <b>4</b> submissions per day per track.
        Each participant team requires to include information for all team members (affiliation and email) during evaluation server registration.
        Multi-account submission from one team is not allowed. Once detected, performance from those accounts will not be taken into consideration for final leaderboard.
    </li>
    <li><b>Q:</b> How to submit source codes per the requirement, and when?<br>
        <b>A:</b> Please create an online repository like github or bitbucket to host your codes and models.
        The organizer will query the repository link for each team after the testing phase finished.
        Teams should open their source codes and models before challenge result notification deadline (August 9, PST time).
        Team without such information will not be considered in the final ranking.
    </li>
    <li><b>Q:</b> Why evaluation server only evaluate the accuracy for the detection track?<br>
        <b>A:</b> Due to evaluation server limitation, the detection track will only evaluate and rank by accuracy (mAP etc).
        After final submission, we require each team to submit their run-time cost (FLOPs and model-size) information. We will build a metric like performance-per-FLOPs (PPF) to rank submissions in this track.
    </li>
    <li><b>Q:</b> How to indicate which submission is final?<br>
        <b>A:</b> You may add a postfix like "final" into your final submission name to indicate that. If you do not indicate that, we will by default use your best performed submissions in the test-dev evaluation as the final submission.
    </li>
    <!--li><b>Q:</b> <br>
        <b>A:</b>
    </li-->
  </ul>
</div>
</div>

<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
<ul>
<li>Training data release: <span style='color: #a00'>June 28, 2019</span></li>
<li>Testing data release: <span style='color: #a00'>July 26, 2019</span></li>
<li>Result submission deadline: <span style='color: #a00'>August 2, 2019</span></li>
<li>Result notification: <span style='color: #a00'> August 9, 2019</span></li>
<li>Challenge paper submission: <span style='color: #a00'> August 15, 2019</span></li>
<li>Acceptance notification: <span style='color: #a00'> August 23, 2019</span></li>
<li>Camera Ready: <span style='color: #a00'> August 30, 2019</span></li>
<li>Workshop: <span style='color: #a00'> October 27, 2019</span></li>
</ul>
</div>
<p><br /></p>
</div>

    <div class="row">
      <div class="col-xs-12">
        <h2>References</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <ol>
        <li>K Ullas Karanth, James D Nichols, N Samba Kumar, et al. Tigers and their prey: predicting carnivore densities from prey abundance. PNAS 101, 14 (2004),4854–4858.</li>
        <li>K Ullas Karanth, James D Nichols, N Samba Kumar, and James E Hines. Assessing tiger population dynamics using photographic capture–recapture sampling. Ecology 87, 11 (2006), 2925–2937.</li>
        <li>Alexander Freytag, Erik Rodner, Marcel Simon, Alexander Loos, et al. Chimpanzee faces in the wild: Log-euclidean cnns for predicting identities and attributes of primates. In German Conference on Pattern Recognition, 2016.</li>
        <li>Matthias Körschens, Björn Barz, and Joachim Denzler. Towards automatic identification of elephants in the wild. arXiv preprint arXiv:1812.04418(2018).</li>
        <li>Andrei Polzounov, Ilmira Terpugova, Deividas Skiparis, and Andrei Mihai. Right whale recognition using convolutional neural networks. arXiv preprint arXiv:1604.05605(2016).</li>
	    </ol>
      </div>
    </div>
    <p><br /></p>
    <p><strong>Contact</strong>: <span style='color: #0000FF'>cvwc2019 AT hotmail.com</span>. Any question related to the workshop such as paper submission, challenge participation, etc, please feel free to send email to the contact mailbox. </p>
    <p><br /></p>
  </div>
</body>
</html>
